{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/piechart-QA.jsonl to s3://llava-ue1/data/piechart-QA.jsonl                \n",
      "upload: data/piechart-QA/.ipynb_checkpoints/image_2-checkpoint.png to s3://llava-ue1/data/piechart-QA/.ipynb_checkpoints/image_2-checkpoint.png\n",
      "upload: data/piechart-QA/.ipynb_checkpoints/image_39-checkpoint.png to s3://llava-ue1/data/piechart-QA/.ipynb_checkpoints/image_39-checkpoint.png\n",
      "upload: data/piechart-QA/.ipynb_checkpoints/image_25-checkpoint.png to s3://llava-ue1/data/piechart-QA/.ipynb_checkpoints/image_25-checkpoint.png\n",
      "upload: data/piechart-QA/.ipynb_checkpoints/image_10-checkpoint.png to s3://llava-ue1/data/piechart-QA/.ipynb_checkpoints/image_10-checkpoint.png\n",
      "upload: data/piechart-QA/.ipynb_checkpoints/image_31-checkpoint.png to s3://llava-ue1/data/piechart-QA/.ipynb_checkpoints/image_31-checkpoint.png\n",
      "upload: data/piechart-QA/.ipynb_checkpoints/image_38-checkpoint.png to s3://llava-ue1/data/piechart-QA/.ipynb_checkpoints/image_38-checkpoint.png\n",
      "upload: data/piechart-QA/.ipynb_checkpoints/image_3-checkpoint.png to s3://llava-ue1/data/piechart-QA/.ipynb_checkpoints/image_3-checkpoint.png\n",
      "upload: data/piechart-QA/.ipynb_checkpoints/image_4-checkpoint.png to s3://llava-ue1/data/piechart-QA/.ipynb_checkpoints/image_4-checkpoint.png\n",
      "upload: data/piechart-QA/.ipynb_checkpoints/image_7-checkpoint.png to s3://llava-ue1/data/piechart-QA/.ipynb_checkpoints/image_7-checkpoint.png\n",
      "upload: data/piechart-QA/.ipynb_checkpoints/image_1-checkpoint.png to s3://llava-ue1/data/piechart-QA/.ipynb_checkpoints/image_1-checkpoint.png\n",
      "upload: data/piechart-QA/image_1.png to s3://llava-ue1/data/piechart-QA/image_1.png\n",
      "upload: data/piechart-QA/.ipynb_checkpoints/image_6-checkpoint.png to s3://llava-ue1/data/piechart-QA/.ipynb_checkpoints/image_6-checkpoint.png\n",
      "upload: data/piechart-QA/image_10.png to s3://llava-ue1/data/piechart-QA/image_10.png\n",
      "upload: data/piechart-QA/image_12.png to s3://llava-ue1/data/piechart-QA/image_12.png\n",
      "upload: data/piechart-QA/image_11.png to s3://llava-ue1/data/piechart-QA/image_11.png\n",
      "upload: data/piechart-QA/.ipynb_checkpoints/image_5-checkpoint.png to s3://llava-ue1/data/piechart-QA/.ipynb_checkpoints/image_5-checkpoint.png\n",
      "upload: data/piechart-QA/.ipynb_checkpoints/image_40-checkpoint.png to s3://llava-ue1/data/piechart-QA/.ipynb_checkpoints/image_40-checkpoint.png\n",
      "upload: data/piechart-QA/image_14.png to s3://llava-ue1/data/piechart-QA/image_14.png\n",
      "upload: data/piechart-QA/image_13.png to s3://llava-ue1/data/piechart-QA/image_13.png\n",
      "upload: data/piechart-QA/.ipynb_checkpoints/image_20-checkpoint.png to s3://llava-ue1/data/piechart-QA/.ipynb_checkpoints/image_20-checkpoint.png\n",
      "upload: data/piechart-QA/image_19.png to s3://llava-ue1/data/piechart-QA/image_19.png\n",
      "upload: data/piechart-QA/image_2.png to s3://llava-ue1/data/piechart-QA/image_2.png\n",
      "upload: data/piechart-QA/image_16.png to s3://llava-ue1/data/piechart-QA/image_16.png\n",
      "upload: data/piechart-QA/image_18.png to s3://llava-ue1/data/piechart-QA/image_18.png\n",
      "upload: data/piechart-QA/image_20.png to s3://llava-ue1/data/piechart-QA/image_20.png\n",
      "upload: data/piechart-QA/image_15.png to s3://llava-ue1/data/piechart-QA/image_15.png\n",
      "upload: data/piechart-QA/image_17.png to s3://llava-ue1/data/piechart-QA/image_17.png\n",
      "upload: data/piechart-QA/.ipynb_checkpoints/image_36-checkpoint.png to s3://llava-ue1/data/piechart-QA/.ipynb_checkpoints/image_36-checkpoint.png\n",
      "upload: data/piechart-QA/image_21.png to s3://llava-ue1/data/piechart-QA/image_21.png\n",
      "upload: data/piechart-QA/image_23.png to s3://llava-ue1/data/piechart-QA/image_23.png\n",
      "upload: data/piechart-QA/image_22.png to s3://llava-ue1/data/piechart-QA/image_22.png\n",
      "upload: data/piechart-QA/image_3.png to s3://llava-ue1/data/piechart-QA/image_3.png\n",
      "upload: data/piechart-QA/image_25.png to s3://llava-ue1/data/piechart-QA/image_25.png\n",
      "upload: data/piechart-QA/image_29.png to s3://llava-ue1/data/piechart-QA/image_29.png\n",
      "upload: data/piechart-QA/image_32.png to s3://llava-ue1/data/piechart-QA/image_32.png\n",
      "upload: data/piechart-QA/image_26.png to s3://llava-ue1/data/piechart-QA/image_26.png\n",
      "upload: data/piechart-QA/image_30.png to s3://llava-ue1/data/piechart-QA/image_30.png\n",
      "upload: data/piechart-QA/image_24.png to s3://llava-ue1/data/piechart-QA/image_24.png\n",
      "upload: data/piechart-QA/image_34.png to s3://llava-ue1/data/piechart-QA/image_34.png\n",
      "upload: data/piechart-QA/image_27.png to s3://llava-ue1/data/piechart-QA/image_27.png\n",
      "upload: data/piechart-QA/image_35.png to s3://llava-ue1/data/piechart-QA/image_35.png\n",
      "upload: data/piechart-QA/image_28.png to s3://llava-ue1/data/piechart-QA/image_28.png\n",
      "upload: data/piechart-QA/image_39.png to s3://llava-ue1/data/piechart-QA/image_39.png\n",
      "upload: data/piechart-QA/image_33.png to s3://llava-ue1/data/piechart-QA/image_33.png\n",
      "upload: data/piechart-QA/image_31.png to s3://llava-ue1/data/piechart-QA/image_31.png\n",
      "upload: data/piechart-QA/image_37.png to s3://llava-ue1/data/piechart-QA/image_37.png\n",
      "upload: data/piechart-QA/image_38.png to s3://llava-ue1/data/piechart-QA/image_38.png\n",
      "upload: data/piechart-QA/image_4.png to s3://llava-ue1/data/piechart-QA/image_4.png\n",
      "upload: data/piechart-QA/image_5.png to s3://llava-ue1/data/piechart-QA/image_5.png\n",
      "upload: data/piechart-QA/image_40.png to s3://llava-ue1/data/piechart-QA/image_40.png\n",
      "upload: data/piechart-QA/image_8.png to s3://llava-ue1/data/piechart-QA/image_8.png\n",
      "upload: data/piechart-QA/image_36.png to s3://llava-ue1/data/piechart-QA/image_36.png\n",
      "upload: data/piechart-QA/image_9.png to s3://llava-ue1/data/piechart-QA/image_9.png\n",
      "upload: data/piechart-QA/image_7.png to s3://llava-ue1/data/piechart-QA/image_7.png\n",
      "upload: data/piechart-QA/image_6.png to s3://llava-ue1/data/piechart-QA/image_6.png\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync ./data/ s3://llava-ue1/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaVA'...\n",
      "remote: Enumerating objects: 2247, done.\u001b[K\n",
      "remote: Counting objects: 100% (1271/1271), done.\u001b[K\n",
      "remote: Compressing objects: 100% (532/532), done.\u001b[K\n",
      "remote: Total 2247 (delta 874), reused 784 (delta 739), pack-reused 976\u001b[K\n",
      "Receiving objects: 100% (2247/2247), 13.86 MiB | 40.10 MiB/s, done.\n",
      "Resolving deltas: 100% (1434/1434), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/haotian-liu/LLaVA.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting LLaVA/finetune-lora-piechart-QA.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile LLaVA/finetune-lora-piechart-QA.sh\n",
    "\n",
    "#!/bin/bash\n",
    "export WANDB_MODE=offline\n",
    "\n",
    "cd /opt/ml/code\n",
    "pip install -e . --no-deps\n",
    "\n",
    "deepspeed llava/train/train_mem.py \\\n",
    "    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\n",
    "    --deepspeed ./scripts/zero3.json \\\n",
    "    --model_name_or_path liuhaotian/llava-v1.5-7b \\\n",
    "    --version v1 \\\n",
    "    --data_path /opt/ml/input/data/piechart/piechart-QA.jsonl \\\n",
    "    --image_folder /opt/ml/input/data/piechart/piechart-QA \\\n",
    "    --vision_tower openai/clip-vit-large-patch14-336 \\\n",
    "    --mm_projector_type mlp2x_gelu \\\n",
    "    --mm_vision_select_layer -2 \\\n",
    "    --mm_use_im_start_end False \\\n",
    "    --mm_use_im_patch_token False \\\n",
    "    --image_aspect_ratio pad \\\n",
    "    --group_by_modality_length True \\\n",
    "    --bf16 True \\\n",
    "    --output_dir /opt/ml/checkpoints/$(job_id) \\\n",
    "    --num_train_epochs 5 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 50000 \\\n",
    "    --save_total_limit 1 \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --tf32 True \\\n",
    "    --model_max_length 2048 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --dataloader_num_workers 4 \\\n",
    "    --lazy_preprocess True \\\n",
    "    --report_to wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::348052051973:role/service-role/SageMaker-ExecutionRole-20231205T130265\n",
      "sagemaker bucket: llava-ue1\n",
      "sagemaker session region: us-east-1\n",
      "data uri: s3://llava-ue1/data\n"
     ]
    }
   ],
   "source": [
    "# Initialize sagemaker session and get the training data s3 uri\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import sagemaker.huggingface\n",
    "import os\n",
    "\n",
    "ROLE = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "BUCKET = \"llava-ue1\"\n",
    "PREFIX = \"data\"\n",
    "s3uri = os.path.join(\"s3://\", BUCKET, PREFIX)\n",
    "print(f\"sagemaker role arn: {ROLE}\")\n",
    "print(f\"sagemaker bucket: {BUCKET}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"data uri: {s3uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llava-v15-7b-task-lora-2024-02-17-07-33-50\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "job_id = \"llava-v15-7b-task-lora-\"+strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = {\n",
    "        'job_id': job_id\n",
    "}\n",
    "\n",
    "# Define metrics definitions, such metrics will be extracted from training script's printed logs and send to cloudwatch\n",
    "metric_definitions=[\n",
    "        {'Name': 'loss', 'Regex': \"'loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'learning_rate', 'Regex': \"'learning_rate': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'epoch', 'Regex': \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'train_runtime', 'Regex': \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'train_samples_per_second', 'Regex': \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'train_steps_per_second', 'Regex': \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'train_loss', 'Regex': \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point the training data to the s3 uri. Use FastFile to \"mount\" the s3 files directly instead of copying to local disk\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "training_input = TrainingInput(\n",
    "    s3_data_type='S3Prefix', # Available Options: S3Prefix | ManifestFile | AugmentedManifestFile\n",
    "    s3_data=s3uri,\n",
    "    distribution='FullyReplicated', # Available Options: FullyReplicated | ShardedByS3Key \n",
    "    input_mode='FastFile'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: llava-v15-7b-task-lora-2024-02-17-07-33-50\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "instance_type = 'ml.g5.12xlarge'\n",
    "use_spot_instances = True\n",
    "max_run=600\n",
    "max_wait = 1200 if use_spot_instances else None\n",
    "\n",
    "output_uri = os.path.join(\"s3://\", BUCKET, job_id, \"output\")\n",
    "checkpoint_uri = os.path.join(\"s3://\", BUCKET, job_id, \"checkpoints\")\n",
    "\n",
    "\n",
    "huggingface_estimator = HuggingFace(entry_point='finetune-lora-piechart-QA.sh',\n",
    "                                    source_dir='./LLaVA',\n",
    "                                    instance_type=instance_type,\n",
    "                                    instance_count=1,\n",
    "                                    py_version='py310',\n",
    "                                    image_uri='348052051973.dkr.ecr.us-east-1.amazonaws.com/llava-training:latest',\n",
    "                                    role=ROLE,\n",
    "                                    metric_definitions = metric_definitions,\n",
    "                                    environment=environment,\n",
    "                                    use_spot_instances=use_spot_instances,\n",
    "                                    max_run=max_run,\n",
    "                                    max_wait=max_wait,\n",
    "                                    output_path=output_uri,\n",
    "                                    checkpoint_s3_uri=checkpoint_uri,\n",
    "                                   )\n",
    "\n",
    "huggingface_estimator.fit({'piechart': training_input}, job_name=job_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
